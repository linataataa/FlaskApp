{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98570b76-3397-4a7f-aa09-ed0dd05afeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c989a08-81b5-4ce3-a530-3e260056ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Churn Prediction\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbb57aa6-31ff-4beb-b62d-5c50f0a4a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "train_data = spark.read.csv(\"C:/Users/khoul/Dropbox/PC/Downloads/kafka-spark-streaming-integration-master/kafka-spark-streaming-integration-master/kafka/src/main/resources/churn-bigml-80.csv\", header=True, inferSchema=True)\n",
    "test_data = spark.read.csv(\"C:/Users/khoul/Dropbox/PC/Downloads/kafka-spark-streaming-integration-master/kafka-spark-streaming-integration-master/kafka/src/main/resources/churn-bigml-20.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf8d3a82-d20b-4a74-9c06-7ea82a0be01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la colonne Churn en string\n",
    "train_data = train_data.withColumn(\"Churn\", train_data[\"Churn\"].cast(\"string\"))\n",
    "test_data = test_data.withColumn(\"Churn\", test_data[\"Churn\"].cast(\"string\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9b8ed9a-3ff1-4785-a696-9db25de1c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertir la colonne Churn en numérique\n",
    "indexer = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
    "train_data = indexer.fit(train_data).transform(train_data)\n",
    "test_data = indexer.fit(test_data).transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef803120-7520-4616-aa59-dedbdf9f61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertir les variables catégorielles en numériques\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(train_data) for column in [\"State\", \"International plan\", \"Voice mail plan\"]]\n",
    "for indexer in indexers:\n",
    "    train_data = indexer.transform(train_data)\n",
    "    test_data = indexer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a26d7ba2-7713-430f-9acb-5fc185b234af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un vecteur de features\n",
    "feature_columns = [\"Account length\", \"Area code\", \"Number vmail messages\", \"Total day minutes\", \"Total day calls\",\n",
    "                   \"Total day charge\", \"Total eve minutes\", \"Total eve calls\", \"Total eve charge\", \"Total night minutes\",\n",
    "                   \"Total night calls\", \"Total night charge\", \"Total intl minutes\", \"Total intl calls\", \"Total intl charge\",\n",
    "                   \"Customer service calls\", \"State_index\", \"International plan_index\", \"Voice mail plan_index\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d01256b1-26ea-40b4-80e8-f77e71a1ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entraîner le modèle GBT avec maxBins augmenté\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxBins=60)\n",
    "gbt_model = gbt.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e2da4cf-79bf-42d9-b6cc-24ae2498a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "predictions = gbt_model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea8f37-4a39-448e-ba90-458d67a2fd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3c270ff-5135-4b3b-a2be-fbb60efc97c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9003220463746787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Évaluer les performances du modèle\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1276f3f9-3516-4f32-bbac-22e4d114d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "172a2d4b-0656-4cd4-8e90-d84fda414fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9003220463746787\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8406b92a-216e-4649-b918-5ac81fc3ac8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[State: string, Account length: int, Area code: int, International plan: string, Voice mail plan: string, Number vmail messages: int, Total day minutes: double, Total day calls: int, Total day charge: double, Total eve minutes: double, Total eve calls: int, Total eve charge: double, Total night minutes: double, Total night calls: int, Total night charge: double, Total intl minutes: double, Total intl calls: int, Total intl charge: double, Customer service calls: int, Churn: string, label: double, State_index: double, International plan_index: double, Voice mail plan_index: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53e10b-2dfa-4a22-8f20-f105f6b2deea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff5b0839-33c8-433d-88ab-4e741ded96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "gbt_model.write().overwrite().save(\"modelnew.pkl\")  # Replace \"model99\" with your desired filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "256aed41-f311-40c7-8d94-9433012452be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+---------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+----------------------+-----+-----+-----------+------------------------+---------------------+--------------------+--------------------+--------------------+----------+\n",
      "|State|Account length|Area code|International plan|Voice mail plan|Number vmail messages|Total day minutes|Total day calls|Total day charge|Total eve minutes|Total eve calls|Total eve charge|Total night minutes|Total night calls|Total night charge|Total intl minutes|Total intl calls|Total intl charge|Customer service calls|Churn|label|State_index|International plan_index|Voice mail plan_index|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+---------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+----------------------+-----+-----+-----------+------------------------+---------------------+--------------------+--------------------+--------------------+----------+\n",
      "|   LA|           117|      408|                No|             No|                    0|            184.5|             97|           31.37|            351.6|             80|           29.89|              215.8|               90|              9.71|               8.7|               4|             2.35|                     1|false|  0.0|       49.0|                     0.0|                  0.0|[117.0,408.0,0.0,...|[1.30108047599407...|[0.93100052542873...|       0.0|\n",
      "|   IN|            65|      415|                No|             No|                    0|            129.1|            137|           21.95|            228.5|             83|           19.42|              208.8|              111|               9.4|              12.7|               6|             3.43|                     4| true|  1.0|       20.0|                     0.0|                  0.0|[65.0,415.0,0.0,1...|[-1.5088499607014...|[0.04663262489687...|       1.0|\n",
      "|   NY|           161|      415|                No|             No|                    0|            332.9|             67|           56.59|            317.8|             97|           27.01|              160.6|              128|              7.23|               5.4|               9|             1.46|                     4| true|  1.0|        2.0|                     0.0|                  0.0|[161.0,415.0,0.0,...|[0.50476391920063...|[0.73292773628793...|       0.0|\n",
      "|   SC|           111|      415|                No|             No|                    0|            110.4|            103|           18.77|            137.3|            102|           11.67|              189.6|              105|              8.53|               7.7|               6|             2.08|                     2|false|  0.0|       30.0|                     0.0|                  0.0|[111.0,415.0,0.0,...|[1.27274142906341...|[0.92726946229628...|       0.0|\n",
      "|   HI|            49|      510|                No|             No|                    0|            119.3|            117|           20.28|            215.1|            109|           18.28|              178.7|               90|              8.04|              11.1|               1|              3.0|                     1|false|  0.0|       40.0|                     0.0|                  0.0|[49.0,510.0,0.0,1...|[1.49834281258167...|[0.95242416963967...|       0.0|\n",
      "|   AK|            36|      408|                No|            Yes|                   30|            146.3|            128|           24.87|            162.5|             80|           13.81|              129.3|              109|              5.82|              14.5|               6|             3.92|                     0|false|  0.0|       43.0|                     0.0|                  1.0|[36.0,408.0,30.0,...|[1.51040593866865...|[0.95350553148152...|       0.0|\n",
      "|   MI|            65|      415|                No|             No|                    0|            211.3|            120|           35.92|            162.6|            122|           13.82|              134.7|              118|              6.06|              13.2|               5|             3.56|                     3|false|  0.0|       14.0|                     0.0|                  0.0|[65.0,415.0,0.0,2...|[1.39321279906750...|[0.94193786591229...|       0.0|\n",
      "|   ID|           119|      415|                No|             No|                    0|            159.1|            114|           27.05|            231.3|            117|           19.66|              143.2|               91|              6.44|               8.8|               3|             2.38|                     5| true|  1.0|       16.0|                     0.0|                  0.0|[119.0,415.0,0.0,...|[-1.3266051058986...|[0.06579142527751...|       1.0|\n",
      "|   VA|            10|      408|                No|             No|                    0|            186.1|            112|           31.64|            190.2|             66|           16.17|              282.8|               57|             12.73|              11.4|               6|             3.08|                     2|false|  0.0|        3.0|                     0.0|                  0.0|[10.0,408.0,0.0,1...|[1.28125617724676...|[0.92840962170362...|       0.0|\n",
      "|   WI|            68|      415|                No|             No|                    0|            148.8|             70|            25.3|            246.5|            164|           20.95|              129.8|              103|              5.84|              12.1|               3|             3.27|                     3|false|  0.0|        9.0|                     0.0|                  0.0|[68.0,415.0,0.0,1...|[1.48487141443755...|[0.95118833816692...|       0.0|\n",
      "|   MN|            74|      510|                No|            Yes|                   33|            193.7|             91|           32.93|            246.1|             96|           20.92|              138.0|               92|              6.21|              14.6|               3|             3.94|                     2|false|  0.0|        1.0|                     0.0|                  1.0|[74.0,510.0,33.0,...|[1.39462142929757...|[0.94209175278700...|       0.0|\n",
      "|   HI|            85|      415|                No|             No|                    0|            235.8|            109|           40.09|            157.2|             94|           13.36|              188.2|               99|              8.47|              12.0|               3|             3.24|                     0|false|  0.0|       40.0|                     0.0|                  0.0|[85.0,415.0,0.0,2...|[1.47362732846115...|[0.95013357535783...|       0.0|\n",
      "|   MN|            46|      415|                No|             No|                    0|            214.1|             72|            36.4|            164.4|            104|           13.97|              177.5|              113|              7.99|               8.2|               3|             2.21|                     2|false|  0.0|        1.0|                     0.0|                  0.0|[46.0,415.0,0.0,2...|[1.38967951303016...|[0.94155017943935...|       0.0|\n",
      "|   VT|           128|      510|                No|            Yes|                   29|            179.3|            104|           30.48|            225.9|             86|            19.2|              323.0|               78|             14.54|               8.6|               7|             2.32|                     0|false|  0.0|       15.0|                     0.0|                  1.0|[128.0,510.0,29.0...|[1.46223651477340...|[0.94904305130740...|       0.0|\n",
      "|   LA|           155|      415|                No|             No|                    0|            203.4|            100|           34.58|            190.9|            104|           16.23|              196.0|              119|              8.82|               8.9|               4|              2.4|                     0| true|  1.0|       49.0|                     0.0|                  0.0|[155.0,415.0,0.0,...|[1.34721614268561...|[0.93669730533000...|       0.0|\n",
      "|   MT|            73|      415|                No|             No|                    0|            160.1|            110|           27.22|            213.3|             72|           18.13|              174.1|               72|              7.83|              13.0|               4|             3.51|                     0|false|  0.0|       21.0|                     0.0|                  0.0|[73.0,415.0,0.0,1...|[1.25794708834676...|[0.92524857646536...|       0.0|\n",
      "|   ID|            77|      415|                No|             No|                    0|            251.8|             72|           42.81|            205.7|            126|           17.48|              275.2|              109|             12.38|               9.8|               7|             2.65|                     2| true|  1.0|       16.0|                     0.0|                  0.0|[77.0,415.0,0.0,2...|[1.26171490152542...|[0.92576810059481...|       0.0|\n",
      "|   MA|           108|      415|                No|             No|                    0|            178.3|            137|           30.31|            189.0|             76|           16.07|              129.1|              102|              5.81|              14.6|               5|             3.94|                     0|false|  0.0|       23.0|                     0.0|                  0.0|[108.0,415.0,0.0,...|[1.42426016924799...|[0.94524214678340...|       0.0|\n",
      "|   KY|            95|      408|                No|             No|                    0|            135.0|             99|           22.95|            183.6|            106|           15.61|              245.3|              102|             11.04|              12.5|               9|             3.38|                     1|false|  0.0|       44.0|                     0.0|                  0.0|[95.0,408.0,0.0,1...|[1.52393780522585...|[0.95469072189665...|       0.0|\n",
      "|   MI|            36|      510|                No|            Yes|                   29|            281.4|            102|           47.84|            202.2|             76|           17.19|              187.2|              113|              8.42|               9.0|               6|             2.43|                     2|false|  0.0|       14.0|                     0.0|                  1.0|[36.0,510.0,29.0,...|[1.19392609978544...|[0.91589627012514...|       0.0|\n",
      "+-----+--------------+---------+------------------+---------------+---------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+----------------------+-----+-----+-----------+------------------------+---------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "\n",
    "# Load the model\n",
    "loaded_model = GBTClassificationModel.load(\"modelnew.pkl\")\n",
    "\n",
    "# Suppose 'test_data' is your DataFrame containing the data to predict\n",
    "predictions = loaded_model.transform(test_data)\n",
    "# Show the predictions\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ad26673-a4b9-4f14-b418-73955c59e6b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[117.0,408.0,0.0,...|       0.0|\n",
      "|[65.0,415.0,0.0,1...|       1.0|\n",
      "|[161.0,415.0,0.0,...|       0.0|\n",
      "|[111.0,415.0,0.0,...|       0.0|\n",
      "|[49.0,510.0,0.0,1...|       0.0|\n",
      "|[36.0,408.0,30.0,...|       0.0|\n",
      "|[65.0,415.0,0.0,2...|       0.0|\n",
      "|[119.0,415.0,0.0,...|       1.0|\n",
      "|[10.0,408.0,0.0,1...|       0.0|\n",
      "|[68.0,415.0,0.0,1...|       0.0|\n",
      "|[74.0,510.0,33.0,...|       0.0|\n",
      "|[85.0,415.0,0.0,2...|       0.0|\n",
      "|[46.0,415.0,0.0,2...|       0.0|\n",
      "|[128.0,510.0,29.0...|       0.0|\n",
      "|[155.0,415.0,0.0,...|       0.0|\n",
      "|[73.0,415.0,0.0,1...|       0.0|\n",
      "|[77.0,415.0,0.0,2...|       0.0|\n",
      "|[108.0,415.0,0.0,...|       0.0|\n",
      "|[95.0,408.0,0.0,1...|       0.0|\n",
      "|[36.0,510.0,29.0,...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show predictions as a table\n",
    "predictions.select(\"features\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6ca2e42-e7a2-4e6f-91f7-4866b957ef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+---------+------------------+---------------+--------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+--------------------+-------+----+----+----+----+----+-----+----+-----+-----+----+----+-----+----+----+----+----+----+----+-----+\n",
      "|  _c0|           _c1|      _c2|               _c3|            _c4|                 _c5|              _c6|            _c7|             _c8|              _c9|           _c10|            _c11|               _c12|             _c13|              _c14|              _c15|            _c16|             _c17|                _c18|   _c19|_c20|_c21|_c22|_c23|_c24| _c25|_c26| _c27| _c28|_c29|_c30| _c31|_c32|_c33|_c34|_c35|_c36|_c37| _c38|\n",
      "+-----+--------------+---------+------------------+---------------+--------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+--------------------+-------+----+----+----+----+----+-----+----+-----+-----+----+----+-----+----+----+----+----+----+----+-----+\n",
      "|State|Account length|Area code|International plan|Voice mail plan|Number vmail mess...|Total day minutes|Total day calls|Total day charge|Total eve minutes|Total eve calls|Total eve charge|Total night minutes|Total night calls|Total night charge|Total intl minutes|Total intl calls|Total intl charge|Customer service ...|ChurnMO| 147| 415| Yes|  No|   0|157.0|  79|26.69|103.1|  94|8.76|211.8|  96|9.53| 7.1|   6|1.92|   0|false|\n",
      "|   WV|           141|      415|               Yes|            Yes|                  37|            258.6|             84|           43.96|            222.0|            111|           18.87|              326.4|               97|             14.69|              11.2|               5|             3.02|                   0|  False|NULL|NULL|NULL|NULL|NULL| NULL|NULL| NULL| NULL|NULL|NULL| NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL|\n",
      "|   RI|            74|      415|                No|             No|                   0|            187.7|            127|           31.91|            163.4|            148|           13.89|              196.0|               94|              8.82|               9.1|               5|             2.46|                   0|  False|NULL|NULL|NULL|NULL|NULL| NULL|NULL| NULL| NULL|NULL|NULL| NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL|\n",
      "|   IA|           168|      408|                No|             No|                   0|            128.8|             96|            21.9|            104.9|             71|            8.92|              141.1|              128|              6.35|              11.2|               2|             3.02|                   1|  False|NULL|NULL|NULL|NULL|NULL| NULL|NULL| NULL| NULL|NULL|NULL| NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL|\n",
      "|   MT|            95|      510|                No|             No|                   0|            156.6|             88|           26.62|            247.6|             75|           21.05|              192.3|              115|              8.65|              12.3|               5|             3.32|                   3|  False|NULL|NULL|NULL|NULL|NULL| NULL|NULL| NULL| NULL|NULL|NULL| NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL|\n",
      "+-----+--------------+---------+------------------+---------------+--------------------+-----------------+---------------+----------------+-----------------+---------------+----------------+-------------------+-----------------+------------------+------------------+----------------+-----------------+--------------------+-------+----+----+----+----+----+-----+----+-----+-----+----+----+-----+----+----+----+----+----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Chemin du fichier CSV\n",
    "file_path = \"C:/Users/khoul/Dropbox/PC/Downloads/kafka-spark-streaming-integration-master/kafka-spark-streaming-integration-master/spark/Master_dataset.csv\"\n",
    "\n",
    "# Lire le CSV sans spécifier le schéma\n",
    "Newdata = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "\n",
    "# Afficher les premières lignes\n",
    "Newdata.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f7c49-dda1-4f59-9cb7-16890e1e6263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3794dfe5-36e1-4d33-b02b-42b0ef81321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = loaded_model.transform(Newdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3798012-2d34-4ad9-ad33-14183c891151",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2293.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 531.0 failed 1 times, most recent failure: Lost task 0.0 in stage 531.0 (TID 553) (host.docker.internal executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3588/1696403424`: (struct<Account length_double_VectorAssembler_f319b2928594:double,Area code_double_VectorAssembler_f319b2928594:double,Number vmail messages_double_VectorAssembler_f319b2928594:double,Total day minutes:double,Total day calls_double_VectorAssembler_f319b2928594:double,Total day charge:double,Total eve minutes:double,Total eve calls_double_VectorAssembler_f319b2928594:double,Total eve charge:double,Total night minutes:double,Total night calls_double_VectorAssembler_f319b2928594:double,Total night charge:double,Total intl minutes:double,Total intl calls_double_VectorAssembler_f319b2928594:double,Total intl charge:double,Customer service calls_double_VectorAssembler_f319b2928594:double,State_index:double,International plan_index:double,Voice mail plan_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.GeneratedMethodAccessor242.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3588/1696403424`: (struct<Account length_double_VectorAssembler_f319b2928594:double,Area code_double_VectorAssembler_f319b2928594:double,Number vmail messages_double_VectorAssembler_f319b2928594:double,Total day minutes:double,Total day calls_double_VectorAssembler_f319b2928594:double,Total day charge:double,Total eve minutes:double,Total eve calls_double_VectorAssembler_f319b2928594:double,Total eve charge:double,Total night minutes:double,Total night calls_double_VectorAssembler_f319b2928594:double,Total night charge:double,Total intl minutes:double,Total intl calls_double_VectorAssembler_f319b2928594:double,Total intl charge:double,Customer service calls_double_VectorAssembler_f319b2928594:double,State_index:double,International plan_index:double,Voice mail plan_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 20 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2293.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 531.0 failed 1 times, most recent failure: Lost task 0.0 in stage 531.0 (TID 553) (host.docker.internal executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3588/1696403424`: (struct<Account length_double_VectorAssembler_f319b2928594:double,Area code_double_VectorAssembler_f319b2928594:double,Number vmail messages_double_VectorAssembler_f319b2928594:double,Total day minutes:double,Total day calls_double_VectorAssembler_f319b2928594:double,Total day charge:double,Total eve minutes:double,Total eve calls_double_VectorAssembler_f319b2928594:double,Total eve charge:double,Total night minutes:double,Total night calls_double_VectorAssembler_f319b2928594:double,Total night charge:double,Total intl minutes:double,Total intl calls_double_VectorAssembler_f319b2928594:double,Total intl charge:double,Customer service calls_double_VectorAssembler_f319b2928594:double,State_index:double,International plan_index:double,Voice mail plan_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.GeneratedMethodAccessor242.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3588/1696403424`: (struct<Account length_double_VectorAssembler_f319b2928594:double,Area code_double_VectorAssembler_f319b2928594:double,Number vmail messages_double_VectorAssembler_f319b2928594:double,Total day minutes:double,Total day calls_double_VectorAssembler_f319b2928594:double,Total day charge:double,Total eve minutes:double,Total eve calls_double_VectorAssembler_f319b2928594:double,Total eve charge:double,Total night minutes:double,Total night calls_double_VectorAssembler_f319b2928594:double,Total night charge:double,Total intl minutes:double,Total intl calls_double_VectorAssembler_f319b2928594:double,Total intl charge:double,Customer service calls_double_VectorAssembler_f319b2928594:double,State_index:double,International plan_index:double,Voice mail plan_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 20 more\r\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"features\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64a670-1d25-4133-92f5-f7e9c03f5ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
